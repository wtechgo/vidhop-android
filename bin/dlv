#!/bin/bash
# YT-DLP video and channel downloader functions.

dlv_help() {
  echo " Title: dlv"
  echo " Description: Download video(s) and channels."
  echo " Commands:"
  echo "    dlv <URL>     => download video at <URL> plus information (metadata, thumbnail) into /VidHop/videos"
  echo "    dlv -c <URL>  => download comments while doing dlv, appended to metadata.json into /VidHop/videos"
  echo "    dlvi <URL>    => download video information only at <URL>, no mp4 download into /VidHop/metadata/videos"
  echo "    dlvi -c <URL> => download comments while doing dlvi, appended to metadata.json into /VidHop/metadata/videos"
  echo "    dlvpl <URL>   => download video playlist into /VidHop/channels/<CHANNEL_NAME>/<PLAYLIST_NAME>"
  echo "    dlvpli <URL>  => download video playlist information into /VidHop/metadata/channels/<CHANNEL_NAME>/<PLAYLIST_NAME>"
  echo "    dlc <URL>     => download channel, all videos, metadata and thumbnails into /VidHop/channels"
  echo "    dlci <URL>    => download channel information, same as dlc but no video downloads into /VidHop/metadata/channels"
  echo "    dla <URL>     => download audio, as m4a from music videos into /VidHop/music"
  echo "    dlalbum <URL> => download a music album as m4a into /VidHop/music/<ALBUM_NAME>"
  echo "    dlapl <URL>   => download audio playlist into /VidHop/music/<CHANNEL_NAME>/<PLAYLIST_NAME>"
  echo "    dlapli <URL>  => download audio playlist information into /VidHop/metadata/music/<CHANNEL_NAME>/<PLAYLIST_NAME>"
  echo "    dlpod <URL>   => download podcast or audio tape into /VidHop/podcasts"
  echo "    dlt <URL>     => download thumbnail and metadata as jpg into /VidHop/thumbnails"
  echo "    dlv720p <URL>  => download video in 1280x720 (HD)"
  echo "    dlv1080p <URL> => download video in 1920x1080 (FHD)"
  echo "    dlv1440p <URL> => download video in 2560x1440 (QHD)"
  echo "    dlv2160p <URL> => download video in 3840x2160 (4K)"
  echo "    dlfbpost <URL>  => download facebook post metadata, no images into /VidHop/social_media"
  echo "    dltweet <URL>   => download twitter tweet metadata, no images into /VidHop/social_media"
  echo "    dlwebsite <URL> => download page or complete website with images into /VidHop/website"
  echo "    dlw <URL>       => alias for dlwebsite"
}

clear_vars_dlv() {
  [ "$(is_media_download)" = true ] && unset url standalone platform name
  unset json json_file meta_file archive_file has_archive archive_content avatar_json \
    avatar_json_file valid album artist pl_name pl_dir media_file thumb_file channel_dir
  rm "$log_json_file" 2>/dev/null
  return 0
}

remove_url_query_parms() {
  url=${url%?si=*} # remove youtube's ?si= param
}

# Fetch video metadata (yt-dlp), select relevant fields and pretty print (jq).
# Populates global variable $json.
fetch_video_metadata() {
  url="$1"
  json=$(
    yt-dlp \
      $([ "$dl_comments" = true ] && echo -n --write-comments) \
      --cookies "$config_dir/cookies.txt" \
      --dump-single-json "$url" |
      jq '{
                  id: .id,
                  title: .title,
                  webpage_url: .webpage_url,
                  original_url: .original_url,
                  description: .description,
                  uploader: .uploader,
                  uploader_id: .uploader_id,
                  uploader_url: .uploader_url,
                  upload_date: .upload_date,
                  channel: .channel,
                  channel_follower_count: .channel_follower_count,
                  channel_id: .channel_id,
                  channel_url: .channel_url,
                  categories: .categories,
                  tags: .tags,
                  duration: .duration,
                  thumbnail: .thumbnail,
                  comments: .comments,
                  view_count: .view_count,
                  average_rating: .average_rating,
                  age_limit: .age_limit,
                  playable_in_embed: .playable_in_embed,
                  is_live: .is_live,
                  was_live: .was_live,
                  live_status: .live_status,
                  release_timestamp: .release_timestamp,
                  like_count: .like_count,
                  availability: .availability,
                  webpage_url_basename: .webpage_url_basename,
                  webpage_url_domain: .webpage_url_domain,
                  fulltitle: .fulltitle,
                  display_id: .display_id,
                  duration_string: .duration_string,
                  __has_drm: .__has_drm,
                  epoch: .epoch,
                  language: .language,
                  protocol: .protocol,
                  ext: .ext,
                  filesize_approx: .filesize_approx,
                  format: .format,
                  format_id: .format_id,
                  format_note: .format_note,
                  vcodec: .vcodec,
                  resolution: .resolution,
                  width: .width,
                  height: .height,
                  vbr: .vbr,
                  fps: .fps,
                  tbr: .tbr,
                  dynamic_range: .dynamic_range,
                  stretched_ratio: .stretched_ratio,
                  acodec: .acodec,
                  audio_channels: .audio_channels,
                  abr: .abr,
                  asr: .asr
            }'
  )
  ext="$(yt-dlp --print ext "$url")"
}

# SINGLE VIDEO
# ------------

# Download video information of one video: metadata and video thumbnail..
fetch_sanitized_title() {
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --print "%(title).${filename_char_count}s" \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '-' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"
}

fetch_song_filename() {
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --print "%(artist)s - %(title).${filename_char_count}s" \
    --replace-in-metadata "artist,title" '"' "'" \
    --replace-in-metadata "artist,title" '\*' '' \
    --replace-in-metadata "artist,title" '/' '' \
    --replace-in-metadata "artist,title" ':' ' -' \
    --replace-in-metadata "artist,title" '#' '[ht]' \
    --replace-in-metadata "artist,title" '<' '' \
    --replace-in-metadata "artist,title" '>' '' \
    --replace-in-metadata "artist,title" '\?' '' \
    --replace-in-metadata "artist,title" '\\' '' \
    --replace-in-metadata "artist,title" '\|' '-' \
    --replace-in-metadata "artist,title" '\`' "'" \
    --verbose "$url"
}

fetch_subs() {
  unset subs_dir auto_subs_dir english english_auto
  [ "$function" = dlv ] || [ "$function" = dlvi ] &&
    meta_dir_dest="$vids_meta_dir/$platform/$name" &&
    media_dir_dest="$vids_dir"
  [ "$function" = dlpod ] || [ "$function" = dlpodi ] &&
    meta_dir_dest="$podcasts_meta_dir/$platform/$name" &&
    media_dir_dest="$podcasts_dir"
  subs_dir="$meta_dir_dest/subs"
  auto_subs_dir="$subs_dir/auto-generated"

  # subs provided by the uploader. https://www.science.co.il/language/Codes.php
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-subs --convert-subs "srt" --sub-langs "en,nl,fr,de,gr,es,it,tr,af,ar,ru,ch" \
    -P "$subs_dir" -o "$name.%(ext)s" \
    --no-download "$url"
  # autogenerated subs
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-auto-subs --convert-subs "srt" --sub-langs "en,nl,fr,de,gr,es,it,tr,af,ar,ru,ch" \
    -P "$auto_subs_dir" -o "$name.%(ext)s" \
    --no-download "$url"

  # The mechanism that forces VLC player to pick the best English subtitles available by default.
  # Extra Info: Media Players prioritize a subs file that is next to a media file,
  # even if there is a subs directory too.
  if [ "$(is_media_download)" = true ] && ! [ "$function" = dlpod ]; then
    english="$subs_dir/${name}.en.srt"
    english_auto="$auto_subs_dir/${name}.en.srt"
    default_subs_en="$media_dir_dest/$platform/$name.en.srt"
    [ -f "$english" ] && /bin/cp "$english" "$default_subs_en"
    ! [ -f "$english" ] && [ -f "$english_auto" ] && /bin/cp "$english_auto" "$default_subs_en"
  fi

  [ "$(is_dir_empty "$auto_subs_dir")" = true ] && rm -rf "$auto_subs_dir"
  [ "$(is_dir_empty "$subs_dir")" = true ] && rm -rf "$subs_dir"
}

print_metadata() {
  echo -e "Metadata JSON:"
  jq 'del(.comments)' <<<"$json"
  print_summary
}

augment_json() {
  [ -z "$meta_file" ] && meta_file="$json_file"
  set_downloaded "$(is_media_download)"
  set_downloaded_date
  [ -n "$media_file" ] && set_metadata_field_json_obj "vh_media_file" "$media_file"
  [ -d "$channel_dir" ] && set_metadata_field_json_obj "vh_channel_dir" "$channel_dir"
  [ -d "$channels_meta_dir/$name/$platform" ] && set_metadata_field_json_obj "vh_channel_meta_dir" "$channels_meta_dir/$name/$platform"
  [ -n "$meta_file" ] && set_metadata_field_json_obj "vh_meta_file" "$meta_file"
  [ -n "$thumb_file" ] && set_metadata_field_json_obj "vh_thumb_file" "$thumb_file"
}

fetch_thumbnail() {
  meta_dir_dest="$1" # Example input for podcasts: "$podcasts_meta_dir/$platform/$name"
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg \
    -P "$meta_dir_dest" -o "thumbnail.%(ext)s" \
    --no-download --verbose "$url"
}

# Download video information. Video metadata and thumbnail.
dlvi() {
  ! [ "$function" = dlv ] && function=dlvi && standalone=true
  clear_vars_dlv
  [ $# = 1 ] && url="$1"
  [ $# = 2 ] && dl_comments=true && url="$2"

  [ "$(is_url "$url")" = false ] && echo "not a URL" && return 1
  platform=$(extract_platform_name "$url")

  fetch_video_metadata "$url" # populates $json
  name="$(fetch_sanitized_title)"
  [ "$(is_platform_facebook)" = true ] && name="$(shorten_text "$name")_$(timestamp)"
  video_meta_dir="$vids_meta_dir/$platform/$name"
  mkdir -p "$video_meta_dir"
  json_file="$video_meta_dir/metadata.json"

  fetch_thumbnail "$video_meta_dir"
  fetch_subs

  add_to_vhistory
  augment_json
  jq <<<"$json" >"$json_file"

  if [ "$(is_media_download)" = false ]; then
    print_metadata
    cd "$vids_meta_dir/$platform/$name" || return
    unset dl_comments function
  fi
}

# Download video. Plus thumbnail and metadata.
dlv() {
  clear_vars_dlv
  [ "$1" = "--help" ] || [ "$1" = "-h" ] && dlv_help && return 0
  [ "$1" = "--comments" ] || [ "$1" = "-c" ] && dl_comments=true && url="$2"
  [ "$#" = 1 ] && url="$1"
  standalone=false
  function=dlv

  [ "$(is_url "$url")" = false ] && echo "not a URL" && return 1
  remove_url_query_parms

  platform=$(extract_platform_name "$url") || return 1
  mkdir "$vids_dir/$platform" 2>/dev/null

  dlvi "$url"
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --no-mtime -P "$vids_dir/$platform" -o "$name.%(ext)s" \
    $([ -n "$res" ] && echo " -S res:$res") \
    --verbose "$url"

  print_metadata
  cd "$vids_dir/$platform" || return
  unset dl_comments function
}

dlv720p() {
  res=720
  dlv "$@"
  unset res
}

dlv1080p() {
  res=1080
  dlv "$@"
  unset res
}

dlv1440p() {
  res=1440
  dlv "$@"
  unset res
}

dlv2160p() {
  res=2160
  dlv "$@"
  unset res
}

# CHANNELS
# --------
# Fetch channel metadata (yt-dlp), select relevant fields and pretty print (jq).
# Populates global variable $json.
fetch_channel_metadata() {
  url="$1"
  archive_args="$(echo "--download-archive $archive_file")"

  json=$(
    yt-dlp --cookies "$config_dir/cookies.txt" \
      --dump-single-json $([ "$has_archive" = "true" ] && echo "$archive_args") --verbose "$url" | jq '
       {
         title: .title,
         description: .description,
         webpage_url: .webpage_url,
         original_url: .original_url,
         url: .url,
         uploader: .uploader,
         upload_date: .upload_date,
         channel: .channel,
         channel_url: .channel_url,
         channel_follower_count: .channel_follower_count,
         duration: .duration,
         duration_string: .duration_string,
         thumbnail: .thumbnail,
         like_count: .like_count,
         n_entries: .n_entries,
         categories: .categories,
         tags: .tags,
         __has_drm: .__has_drm,
         availability: .availability,
         display_id: .display_id,
         fulltitle: .fulltitle,
         is_live: .is_live,
         was_live: .was_live,
         playable_in_embed: .playable_in_embed,
         view_count: .view_count,
         average_rating: .average_rating,
         age_limit: .age_limit,
         epoch: .epoch,
         entries: .entries
       }'
  )
  json=$(
    jq 'del(.entries[] | .formats)' <<<"$json" |
      jq 'del(.entries[] | select(.ie_key))' |
      jq 'del(.entries[] | .thumbnails)' |
      jq 'del(.entries[] | .requested_downloads)' |
      jq 'del(.entries[] | .requested_formats)' |
      jq 'del(.entries[] | .subtitles)' |
      jq 'del(.entries[] | .automatic_captions)' |
      jq 'del(.entries[] | nulls)'
  )
}

# With --downloaded-archive, only new videos are downloaded.
# This method merges previous metadata json with the newly scraped json.
merge_json_with_archive() { # called by dlci
  old_json=$(jq '.' "$json_file")
  entries=$(jq -s '.[0].entries += .[1].entries | .[0].entries | sort_by(.date) | reverse' <<<"$old_json" <<<"$json")
  json=$(jq -s '.[0].entries = .[1] | .[0]' <<<"$old_json" <<<"$entries")
}

# Each yt-dlp command that uses downloaded archive file, updates it, adds the new entries.
# As we call yt-dlp multiple times, we only want to have that happen in the last yt-dlp call.
# To solve that, we reset the file to its original values when the program started.
reset_archive_file() {
  echo "$archive_content" >"$archive_file"
}

# Download channel information (metadata and thumbnails of all videos and the avatar image.
dlci() {
  url="$1"
  # Force dlci to only process /videos instead of /videos, /shorts and /live.
  [[ "$url" == *youtu.be* ]] || [[ "$url" == *youtube.com* ]] && [[ ! "$url" == */videos* ]] && old_url="$url" &&
    url="$url/videos" && echo "changed URL from $old_url to $url"
  standalone="true"
  [ -n "$2" ] && standalone="$2"
  [ -n "$3" ] && debug="$3"
  [ -z "$has_archive" ] && has_archive="false"
  function=$([ "$standalone" = "false" ] && echo dlc || echo dlci)
  logInfo "start dlci $(date +'%d-%m-%Y_%H:%M:%S')"

  archive_file="$downloaded_dir/$(url_to_filename "$url")"
  [ "$has_archive" = "false" ] && has_archive=$([ -f "$archive_file" ] && echo true || echo false)
  archive_content=$(cat "$archive_file" 2>/dev/null)

  logInfo "start fetch_channel_metadata"
  fetch_channel_metadata "$url" # populates $json
  logJq "1: after fetch metadata"

  # quit if no (new) entries (channel videos)
  [ "$(jq '.entries | length' <<<"$json")" = 0 ] && echo no new entries from channel fetch &&
    logInfo "downloaded archive filename:\n$archive_file" && return 0
  [ "$has_archive" ] && reset_archive_file

  # Save metadata to the channel directory. Each channel in its own directory.
  name=$(channel_name_from_obj) # if all downloaded before, no entries, no uploader, channelname = null
  [ "$name" = "null" ] && name=$(url_to_channel_name "$url")
  platform=$(extract_platform_name "$url") # yields youtube or bitchute etc.

  mkdir -p "$channels_meta_dir/$name/$platform" 2>/dev/null
  json_file="$channels_meta_dir/$name/$platform/metadata.json"

  # Merge scraped json with previous json on disk.
  if [ -f "$json_file" ]; then
    logInfo "found a previous channel json, starting merge"

    if [ "$(head -n 1 "$json_file")" = "{" ]; then
      merge_json_with_archive
      logJq "2: merge result"
      [ "$has_archive" ] && reset_archive_file
    else
      logInfo "merge error: bad JSON in $json_file, see json.log"
      logJq "2: merge failed: bad JSON"
      json_file_bkp="$channels_meta_dir/$name/$platform/$name.bkp.json"
      jq <<<"$json" >"$json_file_bkp"
      echo "$url" >>"$import_dir/failed_urls.txt"
      url_to_filename "$url" >>"$import_dir/failed_urls.txt"
      return 1
    fi
  else
    logInfo "no JSON file found at $json_file, skipped merge_json_with_archive"
  fi

  ## Download videos thumbnails
  logInfo "fetch thumbnails"
  [ "$has_archive" ] && reset_archive_file
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg --no-download \
    $([ "$has_archive" = "true" ] && echo "--download-archive $archive_file") \
    -P "$channels_meta_dir/$name/$platform" -o "%(title)s.%(ext)s" \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  #  TODO: Selenium scraper doesn't work in Termux.
  #  logInfo "fetch channel avatar img"
  #  # shellcheck disable=SC2154
  #  python "$scrape_channel_avatar_img_py" --channel-url "$url" --channel-name "$name" --channels-meta-dir "$channels_meta_dir"
  #
  #  avatar_json_file="$channels_meta_dir/$name/$platform/avatar_data.json"
  #  logJq "4 start avatar JSON merge"
  #  if [ -f "$avatar_json_file" ]; then
  #    avatar_json=$(jq . "$avatar_json_file")
  #    json=$(jq -s '.[0] += .[1] | .[0]' <<<"$json" <<<"$avatar_json")
  #    sort_upload_date_desc
  #    logJq "3: after avatar merge"
  #  else
  #    logInfo "avatar json file does not exist at $avatar_json_file"
  #  fi

  logInfo "write json metadata to file"
  logJq "4: final JSON written to file"
  add_to_chistory
  augment_json
  jq <<<"$json" >"$json_file"

  if [ "$standalone" = "true" ]; then # if dlc() did not call dlci(), output to screen, file and exit
    logInfo "write video IDs to downloaded archive and print result"
    save_channel_entries_to_archive
    print_metadata
    cd "$channels_meta_dir/$name/$platform" || return
    logInfo "downloaded archive filename: $archive_file"
    unset has_archive
  fi
  return 0
}

# Download all videos of a channel, metadata of the channel and the avatar image.
dlc() {
  clear_vars_dlv
  url="$1"
  [ "$1" = "--ignore-archive" ] && url="$2" && has_archive=false
  standalone=false
  function=dlc

  dlci "$url" "$standalone"
  [ ! $? ] && echo "download channel info failed (function: dlci)" && return
  channel_dir="$channels_dir/$name/$platform"
  set_channel_dir "$channel_dir" # augments $json

  # Download all videos from channel.
  yt-dlp --cookies "$config_dir/cookies.txt" \
    -P "$channel_dir" -o "%(title)s.%(ext)s" \
    $([ "$has_archive" = true ] && echo "--download-archive $archive_file") \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  save_channel_entries_to_archive
  print_metadata
  cd "$channel_dir" || return
}

fetch_playlist_metadata() {
  url="$1"
  archive_args="$(echo "--download-archive $archive_file")"

  json=$(
    yt-dlp --cookies "$config_dir/cookies.txt" \
      --dump-single-json $([ "$has_archive" = "true" ] && echo "$archive_args") --verbose "$url" | jq '
       {
         title: .title,
         description: .description,
         webpage_url: .webpage_url,
         original_url: .original_url,
         url: .url,
         uploader: .uploader,
         upload_date: .upload_date,
         channel: .channel,
         channel_url: .channel_url,
         channel_follower_count: .channel_follower_count,
         duration: .duration,
         duration_string: .duration_string,
         thumbnail: .thumbnail,
         like_count: .like_count,
         n_entries: .n_entries,
         categories: .categories,
         tags: .tags,
         _has_drm: ._has_drm,
         availability: .availability,
         display_id: .display_id,
         fulltitle: .fulltitle,
         is_live: .is_live,
         was_live: .was_live,
         playable_in_embed: .playable_in_embed,
         view_count: .view_count,
         average_rating: .average_rating,
         age_limit: .age_limit,
         epoch: .epoch,
         entries: .entries
       }'
  )
  json=$(
    jq 'del(.entries[] | .formats)' <<<"$json" |
      jq 'del(.entries[] | select(.ie_key))' |
      jq 'del(.entries[] | .thumbnails)' |
      jq 'del(.entries[] | .formats)' |
      jq 'del(.entries[] | .requested_downloads)' |
      jq 'del(.entries[] | .requested_formats)' |
      jq 'del(.entries[] | .subtitles)' |
      jq 'del(.entries[] | .automatic_captions)' |
      jq 'del(.entries[] | nulls)'
  )
}

# Download video playlist information (similar to dlci).
dlvpli() {
  url="$1"
  standalone="$2"
  [ -n "$3" ] && debug="$3"
  [ -z "$has_archive" ] && has_archive="false"
  function=$([ "$standalone" = "false" ] && echo dlvpl || echo dlvpli)
  logInfo "start dlcpi $(date +'%d-%m-%Y_%H:%M:%S')"

  archive_file="$downloaded_dir/$(url_to_filename "$url")"
  [ "$has_archive" = "false" ] && has_archive=$([ -f "$archive_file" ] && echo true || echo false)
  archive_content=$(cat "$archive_file" 2>/dev/null)

  logInfo "start fetch_playlist_metadata"
  fetch_playlist_metadata "$url" # populates $json
  logJq "1: after fetch metadata"

  # quit if no (new) entries (channel videos)
  [ "$(jq '.entries | length' <<<"$json")" = 0 ] && echo no new entries from channel fetch &&
    logInfo "downloaded archive filename:\n$archive_file" && return 0
  [ "$has_archive" ] && reset_archive_file

  # Save metadata to the channel directory. Each channel in its own directory.
  name=$(channel_name_from_obj)            # if all downloaded before, no entries, no uploader, channelname = null
  platform=$(extract_platform_name "$url") # yields youtube or bitchute etc.
  pl_name="$(jq '.entries[0].playlist_title' <<<"$json" | sanitize_filename)"

  mkdir -p "$channels_meta_dir/$name/$platform/$pl_name" 2>/dev/null
  json_file="$channels_meta_dir/$name/$platform/$pl_name/$pl_name.json"

  # Merge scraped json with previous json on disk.
  if [ -f "$json_file" ]; then
    logInfo "found a previous channel json, starting merge"

    if [ "$(head -n 1 "$json_file")" = "{" ]; then
      merge_json_with_archive
      logJq "2: merge result"
      [ "$has_archive" ] && reset_archive_file
    else
      logInfo "merge error: bad JSON in $json_file, see json.log"
      logJq "2: merge failed: bad JSON"
      json_file_bkp="$channels_meta_dir/$name/$platform/$pl_name/$pl_name.bkp.json"
      jq <<<"$json" >"$json_file_bkp"
      echo "$url" >>"$import_dir/failed_urls.txt"
      url_to_filename "$url" >>"$import_dir/failed_urls.txt"
      return 1
    fi
  else
    logInfo "no JSON file found at $json_file, skipped merge_json_with_archive"
  fi

  ## Download videos thumbnails
  logInfo "fetch thumbnails"
  [ "$has_archive" ] && reset_archive_file
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg --no-download \
    $([ "$has_archive" = "true" ] && echo "--download-archive $archive_file") \
    -P "$channels_meta_dir/$name/$platform/$pl_name" -o "%(track_number)s. %(title)s.%(ext)s" \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  logInfo "write json metadata to file"
  logJq "4: final JSON written to file"
  add_to_chistory
  augment_json
  jq <<<"$json" >"$json_file"

  if [ "$standalone" = true ]; then # if dlc() did not call dlci(), output to screen, file and exit
    logInfo "write video IDs to downloaded archive and print result"
    save_channel_entries_to_archive
    print_metadata
    cd "$channels_meta_dir/$name/$platform/$pl_name" || return
    logInfo "downloaded archive filename: $archive_file"
    unset has_archive
  fi
  return 0
}

# Download video playlist.
dlvpl() {
  clear_vars_dlv
  url="$1"
  [ "$1" = "--ignore-archive" ] && url="$2" && has_archive=false
  standalone=false
  function=dlvpl

  dlvpli "$url" "$standalone"
  [ ! $? ] && echo "download channel info failed (function: dlvpli)" && return

  # Download all videos from channel.
  yt-dlp --cookies "$config_dir/cookies.txt" \
    -P "$channels_dir/$name/$platform/$pl_name" -o "%(track_number)s. %(title)s.%(ext)s" \
    $([ "$has_archive" = true ] && echo "--download-archive $archive_file") \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  save_channel_entries_to_archive
  add_to_chistory
  augment_json
  print_metadata
  cd "$channels_dir/$name/$platform" || return
}

fetch_music_metadata() {
  url="$1"
  json=$(
    yt-dlp --cookies "$config_dir/cookies.txt" \
      $([ "$dl_comments" = true ] && echo -n --write-comments) \
      --dump-single-json "$url" |
      jq '{
                  id: .id,
                  webpage_url: .webpage_url,
                  original_url: .original_url,
                  title: .title,
                  description: .description,
                  album: .album,
                  artist: .artist,
                  creator: .creator,
                  track: .track,
                  release_date: .release_date,
                  release_year: .release_year,
                  alt_title: .alt_title,
                  uploader: .uploader,
                  uploader_id: .uploader_id,
                  uploader_url: .uploader_url,
                  upload_date: .upload_date,
                  channel: .channel,
                  channel_follower_count: .channel_follower_count,
                  channel_id: .channel_id,
                  channel_url: .channel_url,
                  categories: .categories,
                  tags: .tags,
                  duration: .duration,
                  duration_string: .duration_string,
                  thumbnail: .thumbnail,
                  view_count: .view_count,
                  average_rating: .average_rating,
                  playable_in_embed: .playable_in_embed,
                  is_live: .is_live,
                  was_live: .was_live,
                  live_status: .live_status,
                  release_timestamp: .release_timestamp,
                  like_count: .like_count,
                  availability: .availability,
                  webpage_url_basename: .webpage_url_basename,
                  webpage_url_domain: .webpage_url_domain,
                  fulltitle: .fulltitle,
                  display_id: .display_id,
                  __has_drm: .__has_drm,
                  epoch: .epoch,
                  language: .language,
                  protocol: .protocol,
                  ext: .ext,
                  filesize_approx: .filesize_approx,
                  format: .format,
                  format_id: .format_id,
                  acodec: .acodec,
                  audio_channels: .audio_channels,
                  abr: .abr,
                  asr: .asr
            }'
  )
  ext=m4a
}

# Download audio. Save only the m4a audio of a video plus metadata and thumbnail.
dla() {
  function=dla
  clear_vars_dlv
  [ $# = 1 ] && url="$1"
  [ $# = 2 ] && dl_comments=true && url="$2"
  mkdir -p "$music_meta_dir" 2>/dev/null

  # download metadata
  name="$(fetch_song_filename | sed 's/NA - //')"
  fetch_music_metadata "$url" # populates json
  # download media
  yt-dlp --cookies "$config_dir/cookies.txt" \
    -f ba --recode m4a --embed-thumbnail --no-mtime --add-metadata --force-overwrites \
    -P "$music_dir" -o "$name.%(ext)s" --verbose "$url"
  # download thumbnail
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg --no-download \
    -P "$music_meta_dir" -o "$name.%(ext)s" --verbose "$url"

  json_file="$music_meta_dir/$name.json"
  add_to_ahistory
  augment_json
  jq <<<"$json" >"$json_file"

  print_metadata
  cd "$music_dir" || return
}

fetch_music_playlist_metadata() {
  url="$1"
  archive_args="$(echo "--download-archive $archive_file")"

  json=$(
    yt-dlp --cookies "$config_dir/cookies.txt" \
      --dump-single-json $([ "$has_archive" = "true" ] && echo "$archive_args") --verbose "$url" | jq '
       {
         title: .title,
         description: .description,
         webpage_url: .webpage_url,
         original_url: .original_url,
         url: .url,
         uploader: .uploader,
         upload_date: .upload_date,
         channel: .channel,
         channel_url: .channel_url,
         channel_follower_count: .channel_follower_count,
         album: .album,
         artist: .artist,
         creator: .creator,
         track: .track,
         release_date: .release_date,
         release_year: .release_year,
         alt_title: .alt_title,
         duration_string: .duration_string,
         thumbnail: .thumbnail,
         like_count: .like_count,
         n_entries: .n_entries,
         categories: .categories,
         tags: .tags,
         _has_drm: ._has_drm,
         availability: .availability,
         display_id: .display_id,
         fulltitle: .fulltitle,
         is_live: .is_live,
         was_live: .was_live,
         playable_in_embed: .playable_in_embed,
         duration: .duration,
         view_count: .view_count,
         average_rating: .average_rating,
         age_limit: .age_limit,
         epoch: .epoch,
         entries: .entries
       }'
  )
  json=$(
    jq 'del(.entries[] | .formats)' <<<"$json" |
      jq 'del(.entries[] | select(.ie_key))' |
      jq 'del(.entries[] | .thumbnails)' |
      jq 'del(.entries[] | .formats)' |
      jq 'del(.entries[] | .requested_downloads)' |
      jq 'del(.entries[] | .requested_formats)' |
      jq 'del(.entries[] | .subtitles)' |
      jq 'del(.entries[] | .automatic_captions)' |
      jq 'del(.entries[] | nulls)'
  )
}

dlaci() {
  url="$1"
  standalone="$2"
  [ -n "$3" ] && debug="$3"
  [ -z "$has_archive" ] && has_archive="false"
  function=$([ "$standalone" = "false" ] && echo dlac || echo dlaci)
  logInfo "start dlapli $(date +'%d-%m-%Y_%H:%M:%S')"

  archive_file="$downloaded_dir/$(url_to_filename "$url")"
  [ "$has_archive" = "false" ] && has_archive=$([ -f "$archive_file" ] && echo true || echo false)
  archive_content=$(cat "$archive_file" 2>/dev/null)

  logInfo "start fetch_playlist_metadata in dlapli"
  fetch_music_playlist_metadata "$url" # populates $json
  logJq "1: after fetch metadata"

  # quit if no (new) entries (channel videos)
  [ "$(jq '.entries | length' <<<"$json")" = 0 ] && echo no new entries from channel fetch &&
    logInfo "downloaded archive filename:\n$archive_file" && return 0
  [ "$has_archive" ] && reset_archive_file

  # Save metadata to the channel directory. Each channel in its own directory.
  name=$(channel_name_from_obj | tr -d "'") # if all downloaded before, no entries, no uploader, channelname = null
  platform=$(extract_platform_name "$url")  # yields youtube or bitchute etc.  - entries[0].artist
  channel_dir="$music_dir/$name"
  channel_meta_dir="$music_meta_dir/$name"
  json_file="$channel_meta_dir/metadata.json"
  mkdir -p "$channel_dir" 2>/dev/nul
  mkdir -p "$channel_meta_dir" 2>/dev/nul

  # Merge scraped json with previous json on disk.
  if [ -f "$json_file" ]; then
    logInfo "found a previous channel json, starting merge"
    if [ "$(head -n 1 "$json_file")" = "{" ]; then
      merge_json_with_archive # TODO: possibly fails for playlists as this was written for channels
      logJq "2: merge result"
      [ "$has_archive" ] && reset_archive_file
    else
      logInfo "merge error: bad JSON in $json_file, see json.log"
      logJq "2: merge failed: bad JSON"
      json_file_bkp="$channel_meta_dir/metadata.bkp.json"
      jq <<<"$json" >"$json_file_bkp"
      echo "$url" >>"$import_dir/failed_urls.txt"
      url_to_filename "$url" >>"$import_dir/failed_urls.txt"
      return 1
    fi
  else
    logInfo "no JSON file found at $json_file, skipped merge_json_with_archive"
  fi

  ## Download videos thumbnails
  logInfo "fetch thumbnails"
  [ "$has_archive" ] && reset_archive_file
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg --no-download \
    $([ "$has_archive" = "true" ] && echo "--download-archive $archive_file") \
    -P "$channel_meta_dir" -o "%(album)s/%(track_number|)s%(track_number&. |)s%(title)s.%(ext)s" \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  logInfo "write json metadata to file"
  logJq "4: final JSON written to file"
  augment_json
  jq <<<"$json" >"$json_file"

  if [ "$standalone" = true ]; then
    logInfo "write video IDs to downloaded archive and print result"
    save_channel_entries_to_archive
    print_metadata
    cd "$channel_meta_dir" || return
    logInfo "downloaded archive filename: $archive_file"
    unset has_archive
  fi
  return 0
}

dlac() {
  clear_vars_dlv
  url="$1"
  [ "$1" = "--ignore-archive" ] && url="$2" && has_archive=false
  standalone=false
  function=dlac

  dlaci "$url" "$standalone"
  [ ! $? ] && echo "download channel info failed (function: dlaci)" && return

  # Download all videos from channel.
  yt-dlp --cookies "$config_dir/cookies.txt" \
    -f ba --recode m4a --embed-thumbnail --no-mtime --add-metadata \
    -P "$channel_dir" -o "%(album)s/%(track_number|)s%(track_number&. |)s%(title)s.%(ext)s" \
    $([ "$has_archive" = true ] && echo "--download-archive $archive_file") \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  save_channel_entries_to_archive
  print_metadata
  cd "$music_dir/$name" || return
}

# Download video playlist information (similar to dlci).
dlapli() {
  url="$1"
  standalone="$2"
  [ -n "$3" ] && debug="$3"
  [ -z "$has_archive" ] && has_archive="false"
  function=$([ "$standalone" = "false" ] && echo dlapl || echo dlapli)
  logInfo "start dlapli $(date +'%d-%m-%Y_%H:%M:%S')"

  archive_file="$downloaded_dir/$(url_to_filename "$url")"
  [ "$has_archive" = "false" ] && has_archive=$([ -f "$archive_file" ] && echo true || echo false)
  archive_content=$(cat "$archive_file" 2>/dev/null)

  logInfo "start fetch_playlist_metadata in dlapli"
  fetch_music_playlist_metadata "$url" # populates $json
  logJq "1: after fetch metadata"

  # quit if no (new) entries (channel videos)
  [ "$(jq '.entries | length' <<<"$json")" = 0 ] && echo no new entries from channel fetch &&
    logInfo "downloaded archive filename:\n$archive_file" && return 0
  [ "$has_archive" ] && reset_archive_file

  # Save metadata to the channel directory. Each channel in its own directory.
  name=$(channel_name_from_obj)            # if all downloaded before, no entries, no uploader, channelname = null
  platform=$(extract_platform_name "$url") # yields youtube or bitchute etc.  - entries[0].artist
  album=$(jq '.entries[0].album' <<<"$json" | tr -d '"')
  artist=$(jq '.entries[0].artist' <<<"$json" | tr -d '"')
  pl_name="$(jq '.entries[0].playlist_title' <<<"$json" | sanitize_filename)"

  if [ "$album" != null ] && [ "$artist" != null ]; then
    pl_dir="$artist/$album"
  else
    pl_dir="$name/$pl_name"
  fi

  mkdir -p "$music_meta_dir/$pl_dir/$pl_name" 2>/dev/null
  json_file="$music_meta_dir/$pl_dir/$pl_name.json"

  # Merge scraped json with previous json on disk.
  if [ -f "$json_file" ]; then
    logInfo "found a previous channel json, starting merge"
    if [ "$(head -n 1 "$json_file")" = "{" ]; then
      merge_json_with_archive # TODO: possibly fails for playlists as this was written for channels
      logJq "2: merge result"
      [ "$has_archive" ] && reset_archive_file
    else
      logInfo "merge error: bad JSON in $json_file, see json.log"
      logJq "2: merge failed: bad JSON"
      json_file_bkp="$music_meta_dir/$pl_dir/$pl_name.bkp.json"
      jq <<<"$json" >"$json_file_bkp"
      echo "$url" >>"$import_dir/failed_urls.txt"
      url_to_filename "$url" >>"$import_dir/failed_urls.txt"
      return 1
    fi
  else
    logInfo "no JSON file found at $json_file, skipped merge_json_with_archive"
  fi

  ## Download videos thumbnails
  logInfo "fetch thumbnails"
  [ "$has_archive" ] && reset_archive_file
  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg --no-download \
    $([ "$has_archive" = "true" ] && echo "--download-archive $archive_file") \
    -P "$music_meta_dir/$pl_dir" -o "%(playlist_index)s. %(title)s.%(ext)s" \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  logInfo "write json metadata to file"
  logJq "4: final JSON written to file"
  augment_json
  jq <<<"$json" >"$json_file"

  if [ "$standalone" = true ]; then # if dlc() did not call dlci(), output to screen, file and exit
    logInfo "write video IDs to downloaded archive and print result"
    save_channel_entries_to_archive
    print_metadata
    cd "$music_meta_dir/$pl_dir" || return
    logInfo "downloaded archive filename: $archive_file"
    unset has_archive
  fi
  return 0
}

# Download video playlist.
dlapl() {
  clear_vars_dlv
  url="$1"
  [ "$1" = "--ignore-archive" ] && url="$2" && has_archive=false
  standalone=false
  function=dlapl

  dlapli "$url" "$standalone"
  [ ! $? ] && echo "download channel info failed (function: dlapli)" && return

  # Download all videos from channel.
  yt-dlp --cookies "$config_dir/cookies.txt" \
    -f ba --recode m4a --embed-thumbnail --no-mtime --add-metadata \
    -P "$music_dir/$pl_dir" -o "%(playlist_index)s. %(title)s.%(ext)s" \
    $([ "$has_archive" = true ] && echo "--download-archive $archive_file") \
    --replace-in-metadata "title" '"' "'" \
    --replace-in-metadata "title" '\*' '' \
    --replace-in-metadata "title" '/' '' \
    --replace-in-metadata "title" ':' ' -' \
    --replace-in-metadata "title" '#' '[ht]' \
    --replace-in-metadata "title" '<' '' \
    --replace-in-metadata "title" '>' '' \
    --replace-in-metadata "title" '\?' '' \
    --replace-in-metadata "title" '\\' '' \
    --replace-in-metadata "title" '\|' '_' \
    --replace-in-metadata "title" '\`' "'" \
    --verbose "$url"

  save_channel_entries_to_archive
  print_metadata
  cd "$music_dir/$pl_dir" || return
}

dlalbumi() {
  dlapli "$@"
}

dlalbum() {
  dlapl "$@"
}

# Download podcast metadata.
dlpodi() {
  ! [ "$function" = dlpod ] && function=dlpodi && standalone=true
  clear_vars_dlv
  [ $# = 1 ] && url="$1"
  [ $# = 2 ] && url="$2"
  [ "$1" = -c ] || [ "$1" = --comments ] && dl_comments=true
  [ "$(is_url "$url")" = false ] && echo "not a URL" && return 1

  name="$(fetch_sanitized_title)"
  platform=$(extract_platform_name "$url")
  podcast_meta_dir="$podcasts_meta_dir/$platform/$name"
  mkdir -p "$podcast_meta_dir" 2>/dev/null
  json_file="$podcast_meta_dir/metadata.json"

  fetch_video_metadata "$url"
  fetch_thumbnail "$podcast_meta_dir"
  fetch_subs
  ext=m4a

  add_to_phistory
  augment_json
  jq <<<"$json" >"$json_file"

  if [ "$(is_media_download)" = false ]; then
    print_metadata
    cd "$podcast_meta_dir"
    unset dl_comments function
  fi
}

# Download podcast. We differentiate podcasts from music.
dlpod() {
  clear_vars_dlv
  function=dlpod
  dl_comments=false
  [ $# = 1 ] && url="$1"
  [ $# = 2 ] && url="$2"
  [ "$1" = -c ] || [ "$1" = --comments ] && dl_comments=true

  dlpodi "$url"

  if [ "$(has_audio_formats)" = true ]; then
    yt-dlp $(insert_cookies_param) \
      -f "ba*[ext=m4a] / ba[ext=mp3]" --embed-thumbnail --no-mtime --add-metadata --force-overwrites \
      -P "$podcasts_dir/$platform" -o "$name.%(ext)s" --verbose "$url"
  else
    echo "default podcast download failed"
    echo "starting fallback strategy:"
    echo "  1. download video"
    echo "  2. extract audio"
    echo "  3. delete video"
    echo "this can take a few minutes..."
    ext="$(yt-dlp $(insert_cookies_param) --print ext "$url")"
    yt-dlp $(insert_cookies_param) \
      --no-mtime --add-metadata --force-overwrites -P "$podcasts_dir/$platform" -o "$name.%(ext)s" --verbose "$url"
    video_file_tmp="$podcasts_dir/$platform/$name.$ext"
    media_file="$podcasts_dir/$platform/$name.mp3"
    thumb_file="$podcast_meta_dir/thumbnail.jpg"
    to-mp3-remove-input "$video_file_tmp"
    embed-thumbnail-into-mp3 "$thumb_file" "$media_file"
  fi

  cd "$podcasts_dir/$platform" || return
  print_metadata
}

# Download thumbnail. Store the thumbnail of a single video plus its metadata.
dlt() {
  function=dlt
  clear_vars_dlv
  [ $# = 1 ] && url="$1"
  [ $# = 2 ] && dl_comments=true && url="$2"

  platform=$(extract_platform_name "$url")
  mkdir -p "$thumbnails_dir/$platform" "$thumbnails_meta_dir/$platform" 2>/dev/null

  name="$(fetch_sanitized_title)"
  json_file="$thumbnails_meta_dir/$platform/$name.json"
  fetch_video_metadata "$url"

  yt-dlp --cookies "$config_dir/cookies.txt" \
    --write-thumbnail --convert-thumbnails jpg --no-download \
    -P "$thumbnails_dir/$platform" -o "$name.%(ext)s" --verbose "$url"

  add_to_thistory
  augment_json
  jq <<<"$json" >"$json_file"

  print_metadata
  cd "$thumbnails_dir/$platform" || return
}

dlfbpost() {
  url="$1" && [ -z "$url" ] && echo "Usage: dlfbpost <URL>" && return
  [ "$(is_url "$url")" = false ] && echo "not a URL" && return 1
  platform="$(extract_platform_name "$url")"
  function=dlfbpost

  echo "fetching FB post..."
  json="$(python "$facebook_scraper" --url "$url")"

  text="$(jq '.text' <<<"$json")"
  [ "$text" = \"null\" ] && echo "post text was null, most likely because the post wasn't public and only available to FB friends" && return
  username=$(jq .username <<<"$json" | tr -d '"')
  shortened_text="$(shorten_text "$text")"
  timestamp=$(timestamp)
  social_media_dir="${social_media_dir}/$platform/${username}/${shortened_text}_$(timestamp)"
  json_file="$social_media_dir/metadata.json"

  mkdir -p "$social_media_dir" 2>/dev/null
  jq . <<<"$json" >"$json_file"
  cd "$social_media_dir"
  jq . <<<"$json"
}

dltweet() {
  url="$1" && [ -z "$url" ] && echo "Usage: dlfbpost <URL>" && return
  [ "$(is_url "$url")" = false ] && echo "not a URL" && return 1
  platform="$(extract_platform_name "$url")"
  function=dltweet

  echo "fetching tweet..."
  json="$(python "$twitter_scraper" --url "$url")"

  text="$(jq '.text' <<<"$json")"
  [ "$text" = \"null\" ] && echo "post text was null, most likely because the post wasn't public and only available to FB friends" && return
  username=$(jq .username <<<"$json" | tr -d '"')
  shortened_text="$(shorten_text "$text")"
  timestamp=$(timestamp)
  social_media_dir="${social_media_dir}/$platform/${username}/${shortened_text}_$(timestamp)"
  json_file="$social_media_dir/metadata.json"

  mkdir -p "$social_media_dir" 2>/dev/null
  jq . <<<"$json" >"$json_file"
  cd "$social_media_dir"
  jq . <<<"$json"
}

dlwebsite() {
  [ -z "$1" ] || [ "$(is_url "$1")" = false ] && echo "not a valid url" && return
  url="$1"
  python "$website_scraper" "$url" "$websites_dir"
  cd "$websites_dir"
}

dlw() {
  dlwebsite "$1"
}

dlcensored() {
  [ ! "$(command -v tor)" ] && echo "tor not installed, run 'pkg install tor'" and try again && return
  [ ! "$(command -v proxychains4)" ] && echo "proxychains-ng not installed, run 'pkg install proxychains-ng'" and try again && return
  [ ! "$(command -v sv)" ] && echo "sv not installed, run 'pkg install sv'" and try again && return
  [ -z "$1" ] && echo "Usage: dlcensored <URL>" && return || url="$1"

  echo "fetching public IP address via https://api.ipify.org..." # show current, public IP
  echo "your IP-address is $(publicip)" && sleep 2

  echo "starting Tor..." && sleep 3
  sv up tor

  echo "fetching anonymous IP address via https://api.ipify.org..."
  ip_tor="$(proxychains4 bash -c "curl 'https://api.ipify.org?format=text'")"
  echo "your anonymous IP-address is $ip_tor (inside the Tor network)" && sleep 3 # shown anonymous IP

  echo "starting video download..."
  proxychains4 bash -c ". vidhop; dlv $url"

  echo "stopping Tor..." && sleep 2
  sv down tor
  echo "download finished"
}

insert_cookies_param () {
  if [ -n "$BROWSER" ]; then
    echo -n "--cookies-from-browser $BROWSER "
  elif [ -f "$config_dir/cookies.txt" ]; then
    echo -n "--cookies $config_dir/cookies.txt "
  fi
}
